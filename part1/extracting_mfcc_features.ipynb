{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad8db2b-ce73-48be-b76c-0d8a9e815742",
   "metadata": {},
   "source": [
    "# 1. Extracting MFCC features from an audio signal, step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d320f-eb7b-4ad7-be59-099a9ed177da",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8768d2c-31b7-4725-b465-431e0246d421",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import scipy.fftpack as fft\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import get_window\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcffd75-4fb3-459c-9001-c87a0bac2e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Play your .wav file\n",
    "\n",
    "You can record your own .wav audio online - https://voice-recorder-online.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb92f3d-fcff-4260-b1f6-da1df6053f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"audios/example3.wav\"  # replace the filepath accordingly\n",
    "ipd.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45394ac5-2ad6-415a-b980-f985daa31abf",
   "metadata": {},
   "source": [
    "### Read the audio signal into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073d426-40c6-421a-b5db-c4a2ea8fc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate, audio = wavfile.read(audio_file)\n",
    "\n",
    "# Convert to mono channel if stereo\n",
    "if len(audio.shape) == 2:\n",
    "    audio = audio.sum(axis=1) / 2\n",
    "\n",
    "# Normalize the audio signal\n",
    "def normalize_audio(audio):\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio\n",
    "\n",
    "audio = normalize_audio(audio)\n",
    "    \n",
    "print(f\"Sample rate: {sample_rate}Hz\")\n",
    "print(f\"Length: {round(len(audio) / sample_rate, 4)}s\")\n",
    "print(f\"Audio signal shape: {audio.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b95e279-63ab-4134-8afd-b569fa5c7101",
   "metadata": {},
   "source": [
    "### Plot the waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d1b98-6cfd-4689-9d71-1f8d1eb138a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(np.linspace(0, len(audio) / sample_rate, num=len(audio)), audio)\n",
    "plt.title(\"Waveform\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a8049-2897-4a9c-98d9-7cac78b7ffc8",
   "metadata": {},
   "source": [
    "### Extract time frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205f50f-92b4-4613-9614-7d858649f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_audio(audio, frame_n_samples, shift_n_samples):\n",
    "    audio = np.pad(audio, int(frame_n_samples / 2), mode='reflect')\n",
    "    frame_num = int((len(audio) - frame_n_samples) / shift_n_samples) + 1\n",
    "    frames = np.zeros((frame_num, frame_n_samples))\n",
    "    \n",
    "    for n in range(frame_num):\n",
    "        frames[n] = audio[n * shift_n_samples: n * shift_n_samples + frame_n_samples]\n",
    "    \n",
    "    return frames\n",
    "\n",
    "shift_size = 10  # ms\n",
    "frame_size = 25  # ms\n",
    "\n",
    "frame_n_samples = np.round(sample_rate * frame_size / 1000).astype(int)\n",
    "shift_n_samples = np.round(sample_rate * shift_size / 1000).astype(int)\n",
    "\n",
    "audio_framed = frame_audio(audio, frame_n_samples, shift_n_samples)\n",
    "print(f\"Framed audio shape: {audio_framed.shape}\")\n",
    "print(f\"Frame - number of samples: {frame_n_samples}\")\n",
    "print(f\"Shift - number of samples: {shift_n_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee958a86-6dc5-4ece-9a6b-36eb609d2a4e",
   "metadata": {},
   "source": [
    "### Plot overlapping time frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca555e-66f1-4b7b-bc9d-d6dccddb6a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 32\n",
    "fig, axes = plt.subplots(2, 2, figsize=(25, 10))\n",
    "plt.axes(axes[0, 0])\n",
    "axes[0, 0].set_ylim(-0.1, 0.1)\n",
    "plt.plot(audio_framed[index])\n",
    "plt.title(f\"{index}th time frame\")\n",
    "plt.grid(True)\n",
    "index += 1\n",
    "plt.axes(axes[0, 1])\n",
    "axes[0, 1].set_ylim(-0.1, 0.1)\n",
    "plt.plot(audio_framed[index])\n",
    "plt.title(f\"{index}th time frame\")\n",
    "plt.grid(True)\n",
    "index += 1\n",
    "plt.axes(axes[1, 0])\n",
    "axes[1, 0].set_ylim(-0.1, 0.1)\n",
    "plt.plot(audio_framed[index])\n",
    "plt.title(f\"{index}th time frame\")\n",
    "plt.grid(True)\n",
    "index += 1\n",
    "plt.axes(axes[1, 1])\n",
    "axes[1, 1].set_ylim(-0.1, 0.1)\n",
    "plt.plot(audio_framed[index])\n",
    "plt.title(f\"{index}th time frame\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f4c4d-2947-4c73-9555-457eb0801c3f",
   "metadata": {},
   "source": [
    "### Convert to frequency domain and compute power spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f809f1a-fa65-4d6f-955b-3b41d162c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = get_window(\"hann\", frame_n_samples, fftbins=True)\n",
    "audio_win = audio_framed * window\n",
    "\n",
    "audio_winT = np.transpose(audio_win)\n",
    "\n",
    "audio_fft = np.empty((int(1 + frame_n_samples // 2), audio_winT.shape[1]), dtype=np.complex64, order='F')\n",
    "\n",
    "for n in range(audio_fft.shape[1]):\n",
    "    audio_fft[:, n] = fft.fft(audio_winT[:, n], axis=0)[:audio_fft.shape[0]]\n",
    "\n",
    "audio_fft = np.transpose(audio_fft)\n",
    "\n",
    "audio_power = np.square(np.abs(audio_fft))\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(audio_power[index])\n",
    "plt.title(f\"{index}th time frame's power spectrum\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845cf740-81bf-4b67-a19c-4c053885e99b",
   "metadata": {},
   "source": [
    "### Compute Mel filter banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aebe6d-6734-4f9c-a62d-3e9a1a7e65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_min = 0\n",
    "freq_high = sample_rate / 2\n",
    "mel_filter_num = 10\n",
    "\n",
    "print(f\"Minimum frequency in the audio signal: {freq_min}\")\n",
    "print(f\"Maximum frequency in the audio signal: {freq_high}\")\n",
    "print()\n",
    "\n",
    "def freq_to_mel(freq):\n",
    "    return 2595.0 * np.log10(1.0 + freq / 700.0)\n",
    "\n",
    "def met_to_freq(mels):\n",
    "    return 700.0 * (10.0**(mels / 2595.0) - 1.0)\n",
    "\n",
    "def get_filter_points(fmin, fmax, mel_filter_num, fft_size, sample_rate=44100):\n",
    "    fmin_mel = freq_to_mel(fmin)\n",
    "    fmax_mel = freq_to_mel(fmax)\n",
    "    \n",
    "    print(\"MEL min: {0}\".format(fmin_mel))\n",
    "    print(\"MEL max: {0}\".format(fmax_mel))\n",
    "    \n",
    "    mels = np.linspace(fmin_mel, fmax_mel, num=mel_filter_num+2)\n",
    "    freqs = met_to_freq(mels)\n",
    "    \n",
    "    return np.floor((fft_size + 1) / sample_rate * freqs).astype(int), freqs\n",
    "\n",
    "filter_points, mel_freqs = get_filter_points(freq_min, freq_high, mel_filter_num, frame_n_samples, sample_rate=44100)\n",
    "print()\n",
    "print(\"Mel filter points:\")\n",
    "filter_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18609bc-04cd-49e6-b34c-b3c6cf4de19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filters(filter_points, fft_size):\n",
    "    filters = np.zeros((len(filter_points)-2,int(fft_size/2+1)))\n",
    "    \n",
    "    for n in range(len(filter_points)-2):\n",
    "        filters[n, filter_points[n] : filter_points[n + 1]] = np.linspace(0, 1, filter_points[n + 1] - filter_points[n])\n",
    "        filters[n, filter_points[n + 1] : filter_points[n + 2]] = np.linspace(1, 0, filter_points[n + 2] - filter_points[n + 1])\n",
    "    \n",
    "    return filters\n",
    "\n",
    "filters = get_filters(filter_points, frame_n_samples)\n",
    "\n",
    "enorm = 2.0 / (mel_freqs[2:mel_filter_num+2] - mel_freqs[:mel_filter_num])\n",
    "filters *= enorm[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title(\"Mel filters\")\n",
    "for n in range(filters.shape[0]):\n",
    "    plt.plot(filters[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3a7b8-bc24-4c3a-89fe-15e4098b87af",
   "metadata": {},
   "source": [
    "### Apply Mel filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2d5c4-0651-48d6-9d68-371bfce9e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filtered = np.dot(filters, np.transpose(audio_power))\n",
    "audio_log = 10.0 * np.log10(audio_filtered)\n",
    "print(f\"Filtered signal shape: {audio_log.shape}\")\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.title(\"Filtered signal\")\n",
    "plt.imshow(audio_log, origin=\"lower\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315caa2-c2d2-4652-94af-43ab996f4a11",
   "metadata": {},
   "source": [
    "### Extract MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478de458-b7a2-4855-b826-2246399aa808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dct(dct_filter_num, filter_len):\n",
    "    basis = np.empty((dct_filter_num,filter_len))\n",
    "    basis[0, :] = 1.0 / np.sqrt(filter_len)\n",
    "    \n",
    "    samples = np.arange(1, 2 * filter_len, 2) * np.pi / (2.0 * filter_len)\n",
    "\n",
    "    for i in range(1, dct_filter_num):\n",
    "        basis[i, :] = np.cos(i * samples) * np.sqrt(2.0 / filter_len)\n",
    "        \n",
    "    return basis\n",
    "\n",
    "dct_filter_num = 40\n",
    "dct_filters = dct(dct_filter_num, mel_filter_num)\n",
    "# Extract the first 13 coefficients\n",
    "cepstral_coefficents = np.dot(dct_filters, audio_log)[:13,:]\n",
    "\n",
    "print(f\"MFCC features shape: {cepstral_coefficents.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a43b17-40b4-4eb6-a43c-9fd7c54ed37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"MFCC features\")\n",
    "plt.imshow(cepstral_coefficents, cmap=\"gray\", origin=\"lower\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3bbc00-4d42-4ce6-8981-8aef4751929a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
