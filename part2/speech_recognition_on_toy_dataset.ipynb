{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a715891-d7be-4334-9224-4f5231ca03f1",
   "metadata": {},
   "source": [
    "# 3. Training and testing a simple speech recognition model on a toy dataset\n",
    "The input audio signals are a subset of Google's Speech Command Dataset.\n",
    "\n",
    "Download input files from this [Google Drive link](https://drive.google.com/file/d/1LWM97dl8ZIr7k05kVeVKLqje1DUyJjfh/view?usp=sharing) and extract it into this directory (\"part2\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4511dc1e-471a-4459-87ca-fa925754d11c",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc218d0b-c58f-44ec-beaf-c25b5f757e23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5c047-a1c2-4251-8e2f-b2bb65ae1455",
   "metadata": {},
   "source": [
    "### Preprocess the input dataset\n",
    "Extracts 13 MFCC features for every frame in an audio signal.\n",
    "\n",
    "Saves the labels per audio signal and MFCC feature vectors per frame in a .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493edcab-c012-4c9a-aa3e-733c620d7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset\"\n",
    "PREPROCESSED_DATA_PATH = \"preprocessed_data.json\"\n",
    "N_SAMPLES_TO_CONSIDER = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7894cb-f0a8-48ee-96ad-83fb020eb5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 13\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "\n",
    "preprocessed_data = {\n",
    "    \"mapping\": [],\n",
    "    \"labels\": [],\n",
    "    \"MFCCs\": [],\n",
    "    \"files\": []\n",
    "}\n",
    "\n",
    "for i, (dir_path, dir_names, filenames) in enumerate(os.walk(DATASET_PATH)):\n",
    "    if dir_path is not DATASET_PATH:\n",
    "        label = dir_path.split(\"/\")[-1]\n",
    "        preprocessed_data[\"mapping\"].append(label)\n",
    "        print(f\"Processing audios for the word: '{label}'\")\n",
    "\n",
    "        for f in tqdm(filenames):\n",
    "            if not f.endswith(\".wav\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(dir_path, f)\n",
    "            signal, sample_rate = librosa.load(file_path)\n",
    "\n",
    "            if len(signal) >= N_SAMPLES_TO_CONSIDER:\n",
    "                signal = signal[:N_SAMPLES_TO_CONSIDER]\n",
    "                MFCCs = librosa.feature.mfcc(\n",
    "                    y=signal,\n",
    "                    sr=sample_rate,\n",
    "                    n_mfcc=n_mfcc,\n",
    "                    n_fft=n_fft,\n",
    "                    hop_length=hop_length\n",
    "                )\n",
    "\n",
    "                preprocessed_data[\"MFCCs\"].append(MFCCs.T.tolist())\n",
    "                preprocessed_data[\"labels\"].append(i-1)\n",
    "                preprocessed_data[\"files\"].append(file_path)\n",
    "\n",
    "with open(PREPROCESSED_DATA_PATH, \"w\") as f:\n",
    "    json.dump(preprocessed_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf23a4-0ede-4af3-86dc-5739d749bc70",
   "metadata": {},
   "source": [
    "### Train a CNN-based speech recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e4fc1-8707-4479-b48a-f9f7c932e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 5\n",
    "MODEL_PATH = \"model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44baab92-1dfe-49a5-b41f-44412e16cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    X = np.array(data[\"MFCCs\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.2, validation_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n",
    "\n",
    "\n",
    "def build_model(input_shape, learning_rate=0.0001):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, (2, 2), activation='relu',))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    optimiser = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimiser, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_validation, y_validation):\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_validation, y_validation),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", min_delta=0.001, patience=PATIENCE)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(2, figsize=(15, 10))\n",
    "\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "    axs[0].plot(history.history['val_accuracy'], label=\"val_accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy vs. epoch\")\n",
    "\n",
    "    axs[1].plot(history.history[\"loss\"], label=\"loss\")\n",
    "    axs[1].plot(history.history['val_loss'], label=\"val_loss\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Loss\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Loss vs. epoch\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03121b93-b3d2-49a4-b123-371cd6e43284",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(PREPROCESSED_DATA_PATH)\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = split_data(X, y)\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "model = build_model(input_shape)\n",
    "\n",
    "history = train_model(model, X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83771f8b-f227-4f84-89ec-5e799fea1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82648440-c4dc-4b18-993e-61f6a2f40d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {round(test_acc * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68b619-2d12-4726-b92e-881f6b6fea12",
   "metadata": {},
   "source": [
    "### Test the model with your own speech\n",
    "You can record your own voice in a .wav file - https://voice-recorder-online.com/.\n",
    "\n",
    "Try to pronounce one of the words that our model can recognize: \"up\", \"down\", \"right\", \"left\".\n",
    "\n",
    "Look at what our simple speech recognition model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999f95f-a379-49bf-a847-c2751c1ef2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechRecognitionModel:\n",
    "    def __init__(self):\n",
    "        self.model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        with open(PREPROCESSED_DATA_PATH, \"r\") as f:\n",
    "            self.mapping = json.load(f)[\"mapping\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(file_path, num_mfcc=13, n_fft=2048, hop_length=512):\n",
    "        signal, sample_rate = librosa.load(file_path)\n",
    "        if len(signal) >= N_SAMPLES_TO_CONSIDER:\n",
    "            signal = signal[:N_SAMPLES_TO_CONSIDER]\n",
    "            MFCCs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "            return MFCCs.T\n",
    "    \n",
    "    def predict(self, file_path):\n",
    "        MFCCs = self.preprocess(file_path)\n",
    "        MFCCs = MFCCs[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "        predictions = self.model.predict(MFCCs)\n",
    "        predicted_index = np.argmax(predictions)\n",
    "        predicted_keyword = self.mapping[predicted_index]\n",
    "\n",
    "        return predicted_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975ffc7-408f-49a0-bd0f-c8c6b7d14234",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognition_model = SpeechRecognitionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533cb754-1dee-491d-9db5-180fa1a70656",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"test/example1.wav\"  # Replace the filepath accordingly\n",
    "\n",
    "ipd.Audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8d82f-b486-4a07-8ee3-c4b23423b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = speech_recognition_model.predict(audio_file)\n",
    "print(f\"You said {prediction}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
